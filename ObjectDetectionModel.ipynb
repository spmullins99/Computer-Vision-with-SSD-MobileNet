{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import relevant packages/libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "# the following packages are specific to the Raspberry Pi (Rpi)\n",
    "# import RPi.GPIO as GPIO\n",
    "# from time import sleep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distt(x1,y1,x2,y2): # general function for finding Euclidean distance between two points\n",
    "    return np.sqrt(np.square(np.absolute(x2-x1)) + np.square(np.absolute(y2-y1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_to_camera(knownWidth, focalLength, perWidth):\n",
    "    return (knownWidth * focalLength) / perWidth # compute and return the distance of the object from the camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specific to Rpi, set initial state of LEDs \n",
    "# GPIO.setwarnings(False)\n",
    "# GPIO.setmode(GPIO.BOARD)\n",
    "# GPIO.setup(8, GPIO.OUT, initial=GPIO.LOW)\n",
    "# GPIO.setup(10, GPIO.OUT, initial=GPIO.LOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "classNames= [] # variable used for names of class labels\n",
    "classFile = 'coco.names' # text file containing labels\n",
    "with open(classFile,'rt') as f:  # open file and parse class labels\n",
    "    classNames = f.read().rstrip('\\n').split('\\n')\n",
    "\n",
    "configPath = 'ssd_mobilenet_v3_large_coco_2020_01_14.pbtxt' # network configuration\n",
    "weightsPath = 'frozen_inference_graph.pb' # trained weights\n",
    "\n",
    "net = cv2.dnn_DetectionModel(weightsPath,configPath) # create detection model usng trained weights and network configuration\n",
    "# scaling and pixel intensity parameters\n",
    "net.setInputSize(320,320)\n",
    "net.setInputScale(1.0/ 127.5)\n",
    "net.setInputMean((127.5, 127.5, 127.5)) \n",
    "net.setInputSwapRB(True)\n",
    "\n",
    "def getObjects(img,thres,nms,ped_width,cyc_width,focalLength,draw=True,objects=[]): # object detection function\n",
    "    # nms parameter dictates multiple detections and supresses the boundng boxes with non-maximum confidence level\n",
    "    dist_centres = 0 # avoid local variable assignment problems\n",
    "    counter2 = 0 # used to count number of object detected over entire video/webcam duration, initiliase to 0 at every iteration\n",
    "    counter3 = 0 # used for cumulative value of all confidence levels throughout runtime, initiliase to 0 at every iteration\n",
    "    classIds, confs, bbox = net.detect(img,confThreshold=thres,nmsThreshold=nms) # retrieve details about objects detected\n",
    "    if len(objects) == 0: # if user did not define any specific objects to detect \n",
    "        objects = classNames # detect all objects within coco.names file\n",
    "    objectInfo =[] # create variable for object information (confidence level...)\n",
    "    if len(classIds) != 0: # if object detected\n",
    "        cyc_centrex=0 # avoid local variable assignment problems\n",
    "        cyc_centrey=0 # avoid local variable assignment problems\n",
    "        for i in range(classIds.shape[0]): # iterate through class IDs\n",
    "            if classIds[i] == 2: # if object detected is bicycle\n",
    "                counter = i # save specific index for later use \n",
    "                box_temp = bbox[i] # save bounding box coordinates of bicycle\n",
    "                cyc_centrex = box_temp[0]+(box_temp[2]/2) # centre bicycle x coord\n",
    "                cyc_centrey = box_temp[1]+(box_temp[3]/2) # centre bicycle y coord\n",
    "                break\n",
    "        dist_centres = 0 # avoid local variable assignment problems\n",
    "        ped_centrex = 0 # avoid local variable assignment problems\n",
    "        ped_centrey = 0 # avoid local variable assignment problems\n",
    "        for classId, confidence,box in zip(classIds.flatten(),confs.flatten(),bbox):  # iterates through multiple objects\n",
    "            className = classNames[classId - 1] # retrieve class label name\n",
    "            counter2 += 1 # increment by 1 for each object detected\n",
    "            counter3 += confidence # add current confidence value to running total\n",
    "            if className in objects: # if object detected is within user-specified list\n",
    "                objectInfo.append([box, className]) # if object within class list, append name and set of bounding boxes\n",
    "                dist = distance_to_camera(ped_width, focalLength, box[2]) # distance calculation for pedestrian\n",
    "# RASPBERRY PI LED alert\n",
    "#                 if dist < 200 and classIds[i] == 2: # covers any situation where cyclist in frame (multiple objects...)\n",
    "#                     GPIO.output(8, GPIO.HIGH) # light LED 1\n",
    "#                     GPIO.output(10, GPIO.HIGH) # light LED 2\n",
    "#                     sleep(0.5) # ensures LED turns off 0.5 seconds after cyclist(s) have passed, will keep resetting if cyclist still in frame\n",
    "#                     GPIO.output(8, GPIO.LOW) # turn off LED 1\n",
    "#                     GPIO.output(10, GPIO.LOW) # turn off LED 2\n",
    "                    \n",
    "#                 else: # covers situation where only pedestrians have been detected, located within if statement where object has been detected already                    \n",
    "#                     GPIO.output(8, GPIO.HIGH) # light LED 1                  \n",
    "#                     sleep(0.5) # ensures LED turns off 0.5 seconds after pedestrian(s) have passed\n",
    "#                     GPIO.output(8, GPIO.LOW) # turn off LED 1\n",
    "                \n",
    "                ped_centrex = box[0]+(box[2]/2) # centre pedestrian x coord\n",
    "                ped_centrey = box[1]+(box[3]/2) # centre pedestrian y coord\n",
    "                dist_centres = distt(ped_centrex,ped_centrey,cyc_centrex,cyc_centrey) # calculate distance from bicycle to pedestrian\n",
    "                if dist_centres < 150: # if person close enough to bicycle change label to cyclist\n",
    "                    if dist_centres == 0: # avoids bicycle label itself becoming cyclist\n",
    "                        break\n",
    "                    else:\n",
    "                        className = 'cyclist' # change class label to 'cyclist'\n",
    "                        dist = distance_to_camera(cyc_width, focalLength, box[2]) # distance calculation for cyclist, replaces pedestrian distance\n",
    "                if (draw): # initially True\n",
    "                    cv2.rectangle(img,box,color=(0,255,0),thickness=2) # display bounding box\n",
    "                    cv2.putText(img,className.upper(),(box[0]+10,box[1]+30),\n",
    "                    cv2.FONT_HERSHEY_COMPLEX,0.8,(0,255,0),2) # display object class name\n",
    "                    if box[3] > 200: # if box large enough, avoids overcrowding the screen\n",
    "                        cv2.putText(img,str(round(confidence*100,2)),(box[0]+10,box[1]+60),\n",
    "                        cv2.FONT_HERSHEY_COMPLEX,0.8,(0,255,0),2) # display confidence level\n",
    "                        cv2.putText(img,\"%.2f in\" % dist,(box[0]+10,box[1]+90),\n",
    "                        cv2.FONT_HERSHEY_COMPLEX,0.8,(0,255,0),2) # display distance \n",
    "                    \n",
    "    return img,objectInfo,counter2,counter3 # return useful figures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'bbox' is simply just an array of pixel values (top left corner x, top left corner y, width, height) of the bounding boxes of ALL objects within image, '.flatten' collapses matrix into one dimension, zip joins each 1D array of classIds, confs and bbox values together in single tuple, simply just a way to iterate through and extract the relevant information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image, Video, Webcam "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Focal length calibration\n",
    "# KNOWN_DISTANCE = 9 # inches, used for initial calibration, corresponds to fork, spoon, phone, mouse... images\n",
    "# focalLength = (p[2] * KNOWN_DISTANCE) / KNOWN_WIDTH # triangle similarity formula rearranged, average of many of these values used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to run algorithm on STILL IMAGE\n",
    "KNOWN_PEDWIDTH = 20 # human average, adjusted after evaluation and testing\n",
    "KNOWN_CYCWIDTH = 68 # average width of bicycle, adjusted after evaluation and testing\n",
    "focalLength = 827 # calculated in calibration\n",
    "\n",
    "cv2.namedWindow(\"output\", cv2.WINDOW_NORMAL) # Create window with freedom of dimensions\n",
    "image1 = cv2.imread(\"Cyclist1.png\") # read in image\n",
    "image = cv2.resize(image1, (1200, 650)) # Resize image to fit on screen\n",
    "# call 'getObjects' function'on image, threshold value of 0.45, nms value of 0.2, specifiy 'person' and 'bicycle' as objects to be detected\n",
    "result,objectInfo,c2,c3 = getObjects(image,0.45,0.2,KNOWN_PEDWIDTH,KNOWN_CYCWIDTH,focalLength,objects=['person','bicycle']) \n",
    "cv2.imshow(\"Output\", image) # output image to screen\n",
    "cv2.imwrite(\"Cyclist11.jpg\", image) # save resulting image\n",
    "cv2.waitKey(0) # wait for key to be pressed to close window\n",
    "cv2.destroyAllWindows() # appropriately close window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to run algorithm on VIDEO\n",
    "\n",
    "counter1 = 0 # variable used to measure frames per second of algorithm, benchmark value\n",
    "counter2 = 0 # used to count number of object detected over entire video/webcam duration\n",
    "counter3 = 0 # used for cumulative value of all confidence levels throughout runtime\n",
    "KNOWN_PEDWIDTH = 20 \n",
    "KNOWN_CYCWIDTH = 68  \n",
    "focalLength = 827\n",
    "\n",
    "cap = cv2.VideoCapture(0) # create 'VideoCapture' object, 'Cycling_vid_Trim_Trim.avi'\n",
    "# set input paramters (height and width)\n",
    "cap.set(3, 640)\n",
    "cap.set(4, 480)\n",
    "\n",
    "# We need to set resolutions for VideoWriter function convert them from float to integer. \n",
    "frame_width = int(cap.get(3)) \n",
    "frame_height = int(cap.get(4)) \n",
    "size = (frame_width, frame_height) \n",
    "# Below 'VideoWriter' object will create a frame of above, the output is stored as video file\n",
    "vid = cv2.VideoWriter('CycPed.avi', cv2.VideoWriter_fourcc(*'MJPG'), 10, size) # save video of detected objects\n",
    "\n",
    "try:\n",
    "    while True: # constantly reads individual frames \n",
    "        success, img = cap.read() # first variable is 0 or 1 depending on whether video successfully read, second is video itself i.e. image frames\n",
    "        if success == 1: # error handling   \n",
    "            # call 'getObjects' function on iterated frames\n",
    "            result,objectInfo,c1,c2 = getObjects(img,0.45,0.2,KNOWN_PEDWIDTH,KNOWN_CYCWIDTH,focalLength,objects=[]) # \n",
    "            vid.write(img) # write video into file one frame at time \n",
    "            counter1 += 1 # number of frames\n",
    "            counter2 += c1 # number of objects detected within single image\n",
    "            counter3 += c2 # cumulative confidence levels of all objects\n",
    "            meanclass_accuracy = counter3/counter2 # mean classifaction accuracy calculation\n",
    "            cv2.imshow(\"Output\", img) # output images\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'): # run through until user presses 'q'\n",
    "                break\n",
    "        else: # break while loop if image not read successfully\n",
    "            break\n",
    "\n",
    "    cap.release() # release 'VideoCapture' object \n",
    "    vid.release() # release 'VideoWriter' object \n",
    "    cv2.destroyAllWindows() \n",
    "    \n",
    "except Exception as e: # error handling\n",
    "    print(str(e))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
